<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>morl_agents &mdash; MORLBENCH 1.0.1 documentation</title>
    
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="MORLBENCH 1.0.1 documentation" href="../index.html" />
    <link rel="up" title="Module code" href="index.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">MORLBENCH 1.0.1 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for morl_agents</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Created on Nov 19, 2012</span>

<span class="sd">@author: Dominik Meyer &lt;meyerd@mytum.de&gt;</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">helpers</span> <span class="kn">import</span> <span class="n">virtualFunction</span><span class="p">,</span> <span class="n">SaveableObject</span><span class="p">,</span> <span class="n">HyperVolumeCalculator</span><span class="p">,</span> <span class="n">remove_duplicates</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">logging</span> <span class="kn">as</span> <span class="nn">log</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># import neurolab only if it exists in case it is not used and not installed</span>
    <span class="c1"># such that the other agents still work</span>
    <span class="kn">import</span> <span class="nn">neurolab</span> <span class="kn">as</span> <span class="nn">nl</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">log</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Neurolab not installed: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)))</span>
    <span class="n">nl</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># log.basicConfig(level=if my_debug: log.debug)</span>
<span class="n">my_debug</span> <span class="o">=</span> <span class="n">log</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">getEffectiveLevel</span><span class="p">()</span> <span class="o">==</span> <span class="n">log</span><span class="o">.</span><span class="n">DEBUG</span>


<div class="viewcode-block" id="MorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.MorlAgent">[docs]</a><span class="k">class</span> <span class="nc">MorlAgent</span><span class="p">(</span><span class="n">SaveableObject</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A agent that should interface with a MORL problem.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Agent with the MORL problem</span>
<span class="sd">        problem, it will be faced with.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        morl_problem: The already initialized and</span>
<span class="sd">            correctly parametrized problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">([])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span> <span class="o">=</span> <span class="n">morl_problem</span>
        <span class="c1"># Discount Factor is part of the problem, but is used in most algorithms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">morl_problem</span><span class="o">.</span><span class="n">gamma</span>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Learn on the last interaction specified by the</span>
<span class="sd">        action and the reward received.</span>

<span class="sd">        :param t: Interaction cycle we are currently in</span>
<span class="sd">        :param last_state: The last state where we transited from</span>
<span class="sd">        :param action: last interaction action</span>
<span class="sd">        :param reward: received reward vector</span>
<span class="sd">        :param state: next state transited to</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># virtualFunction()</span>

<div class="viewcode-block" id="MorlAgent.decide"><a class="viewcode-back" href="../Agents.html#morl_agents.MorlAgent.decide">[docs]</a>    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decide which action to take in interaction</span>
<span class="sd">        cycle t.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param t: Interaction cycle we are currently in</span>
<span class="sd">        :param state: state we are in</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        action: The action to do next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">virtualFunction</span><span class="p">()</span></div>

<div class="viewcode-block" id="MorlAgent.learn"><a class="viewcode-back" href="../Agents.html#morl_agents.MorlAgent.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Learn from the last interaction, if we have</span>
<span class="sd">        a dynamically learning agent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param t: int Interaction cycle.</span>
<span class="sd">        :param last_state: Last state where we came from</span>
<span class="sd">        :param action: last interaction action</span>
<span class="sd">        :param reward: received reward vector</span>
<span class="sd">        :param state: next state transited to</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">virtualFunction</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="TDMorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.TDMorlAgent">[docs]</a><span class="k">class</span> <span class="nc">TDMorlAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A MORL agent, that uses TD for Policy Evaluation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the TD Policy Evaluation learner for MORL.</span>
<span class="sd">        Scalarization weights have to be given.</span>

<span class="sd">        :param problem: MORL problem.</span>
<span class="sd">        :param scalarization_weights: Reward scalarization weights.</span>
<span class="sd">        :param policy: A static policy, that will be evaluated.</span>
<span class="sd">        :param alpha: Learning rate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TDMorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span> <span class="o">=</span> <span class="n">scalarization_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">scalar_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_V</span><span class="p">[</span><span class="n">last_state</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">scalar_reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_V</span><span class="p">[</span><span class="n">last_state</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39; V: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_V</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">110</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))))</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span><span class="o">.</span><span class="n">decide</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="TDMorlAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.TDMorlAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent! Be careful and save value function first!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SARSAMorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.SARSAMorlAgent">[docs]</a><span class="k">class</span> <span class="nc">SARSAMorlAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A MORL agent, that uses RL.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Reinforcement Learning MORL</span>
<span class="sd">        Agent with the problem description and alpha,</span>
<span class="sd">        the learning rate.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param problem: A MORL problem</span>
<span class="sd">        :param scalarization_weights: a weight vector to scalarize the morl reward.</span>
<span class="sd">        :param alpha: real, the learning rate in each</span>
<span class="sd">            SARSA update step</span>
<span class="sd">        :param epsilon: real, [0, 1] the epsilon factor for</span>
<span class="sd">            the epsilon greedy action selection strategy</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SARSAMorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span> <span class="o">=</span> <span class="n">scalarization_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="c1"># hidden variables for conserving agent state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">last_action</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">scalar_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">last_action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">scalar_reward</span> <span class="o">+</span>
                                                           <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span>
                                                           <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">-</span>
                                                           <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">last_action</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39; Q: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]))[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># action = self._Q[state, :].argmax()</span>
            <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;  took greedy action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">action</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;   took random action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">2.0</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_learned_action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.6</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<div class="viewcode-block" id="SARSAMorlAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.SARSAMorlAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent! Be careful and save learned Q matrix first!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>


<div class="viewcode-block" id="SARSALambdaMorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.SARSALambdaMorlAgent">[docs]</a><span class="k">class</span> <span class="nc">SARSALambdaMorlAgent</span><span class="p">(</span><span class="n">SARSAMorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SARSA with eligibility traces.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lmbda</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SARSALambdaMorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_lmbda</span> <span class="o">=</span> <span class="n">lmbda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="c1"># hidden variables for conserving agent state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e_save</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;SARSALambda_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lmbda</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;e&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;a&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;W=&quot;</span> <span class="o">+</span>\
               <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">last_action</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">scalar_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">scalar_reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_e</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_e</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lmbda</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span> <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39; Q: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)))</span>

<div class="viewcode-block" id="SARSALambdaMorlAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.SARSALambdaMorlAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent! Be careful and save learned Q matrix and the eligibility traces first!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_e_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_e_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>


<div class="viewcode-block" id="QMorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.QMorlAgent">[docs]</a><span class="k">class</span> <span class="nc">QMorlAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A MORL agent, that uses Q learning.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Reinforcement Learning MORL</span>
<span class="sd">        Agent with the problem description and alpha,</span>
<span class="sd">        the learning rate.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param problem: A MORL problem</span>
<span class="sd">        :param scalarization_weights: a weight vector to scalarize the morl reward.</span>
<span class="sd">        :param alpha: real, the learning rate in each</span>
<span class="sd">            Q update step</span>
<span class="sd">        :param epsilon: real, [0, 1] the epsilon factor for</span>
<span class="sd">            the epsilon greedy action selection strategy</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QMorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span> <span class="o">=</span> <span class="n">scalarization_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="c1"># the Q function is only one dimensional, since</span>
        <span class="c1"># we can only be in one state and choose from</span>
        <span class="c1"># two actions in the newcomb problem.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="c1"># self._Q = np.ones(</span>
        <span class="c1">#         (self._morl_problem.n_states, self._morl_problem.n_actions, self._morl_problem.reward_dimension))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># hidden variables for conserving agent state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;scalQ_e&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;a&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;W=&quot;</span> <span class="o">+</span>\
               <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updating the Q-table according to Suttons Q-learning update for multiple</span>
<span class="sd">        objectives</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="n">t</span>
        <span class="c1"># scalar_reward = np.dot(self._scalarization_weights.T, reward)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span>\
            <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39; Q: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])))</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="n">weighted_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">weighted_q</span> <span class="o">==</span> <span class="nb">max</span><span class="p">(</span><span class="n">weighted_q</span><span class="p">))[</span><span class="mi">0</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;  took greedy action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">action</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;   took random action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.6</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="c1"># action_value = np.dot(self._Q[state, :], self._scalarization_weights)</span>
        <span class="c1"># action_value = action_value.ravel()</span>
        <span class="c1"># action_value /= action_value.sum()</span>
        <span class="c1"># return action_value</span>

    <span class="k">def</span> <span class="nf">get_learned_action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.2</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<div class="viewcode-block" id="QMorlAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.QMorlAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent! Be careful and save learned Q matrix first!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>


<div class="viewcode-block" id="PreScalarizedQMorlAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.PreScalarizedQMorlAgent">[docs]</a><span class="k">class</span> <span class="nc">PreScalarizedQMorlAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A MORL agent, that uses Q learning with a scalar</span>
<span class="sd">    value function, scalarizing on every learning step</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Reinforcement Learning MORL</span>
<span class="sd">        Agent with the problem description and alpha,</span>
<span class="sd">        the learning rate.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        :param problem: A MORL problem</span>
<span class="sd">        :param scalarization_weights: a weight vector to scalarize the morl reward.</span>
<span class="sd">        :param alpha: real, the learning rate in each</span>
<span class="sd">            Q update step</span>
<span class="sd">        :param epsilon: real, [0, 1] the epsilon factor for</span>
<span class="sd">            the epsilon greedy action selection strategy</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PreScalarizedQMorlAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span> <span class="o">=</span> <span class="n">scalarization_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;preScalQ_e&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;a&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;W=&quot;</span> <span class="o">+</span>\
               <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># self._learn(0, last_state, self._last_action, reward, state)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Update last action after learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updating the Q-table according to Suttons Q-learning update for multiple</span>
<span class="sd">        objectives</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">scalar_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> \
            <span class="p">(</span><span class="n">scalar_reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39; Q: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:])))</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]))[</span><span class="mi">0</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;  took greedy action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">action</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;   took random action </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_learned_action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<div class="viewcode-block" id="PreScalarizedQMorlAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.PreScalarizedQMorlAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent! Be careful and save learned Q matrix first!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>


<div class="viewcode-block" id="FixedPolicyAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.FixedPolicyAgent">[docs]</a><span class="k">class</span> <span class="nc">FixedPolicyAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An agent, that follows a fixed policy, defined as a morl policy object.</span>
<span class="sd">    No learning implemented.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FixedPolicyAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span> <span class="o">=</span> <span class="n">policy</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span><span class="o">.</span><span class="n">decide</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="FixedPolicyAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.FixedPolicyAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        resets the current agent!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_policy</span><span class="o">.</span><span class="n">_pi</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span></div>


<div class="viewcode-block" id="NFQAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.NFQAgent">[docs]</a><span class="k">class</span> <span class="nc">NFQAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements neural fitted-Q iteration</span>

<span class="sd">    Can be used with scenarios where the reward vector contains only</span>
<span class="sd">    values between -1 and 1.</span>

<span class="sd">    TODO: Currently only morl problems with a cartesian coordinate</span>
<span class="sd">    system as state can be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NFQAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span> <span class="o">=</span> <span class="n">scalarization_weights</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_transistion_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># full transition history (s,a,a&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># input history for NN (s,a)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_hist</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># goal history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_state</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span>

        <span class="c1"># Create network with 2 layers and random initialized</span>
        <span class="c1"># self._net = nl.net.newff([[0, self._morl_problem.n_states], [0, 3]], [20, 20, 1])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_net</span> <span class="o">=</span> <span class="n">nl</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">newff</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">scene_y_dim</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">scene_x_dim</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">]],</span>
                                 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_state</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transistion_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># full transition history (s,a,a&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># input history for NN (s,a)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_hist</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># goal history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_net</span> <span class="o">=</span> <span class="n">nl</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">newff</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">scene_y_dim</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">scene_x_dim</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">]],</span>
                                 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_last_reward</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># state transition and update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_reward</span> <span class="o">=</span> <span class="n">reward</span>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">last_action</span><span class="p">,</span> <span class="n">last_reward</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transistion_history</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">])</span>

        <span class="c1"># Generate training set</span>
        <span class="c1"># self._train_history.append([last_state, last_action])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">_get_position</span><span class="p">(</span><span class="n">last_state</span><span class="p">)),</span> <span class="n">last_action</span><span class="p">]))</span>

        <span class="n">Q_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="c1"># Simulate network</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">_get_position</span><span class="p">(</span><span class="n">state</span><span class="p">)),</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">Q_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_net</span><span class="o">.</span><span class="n">sim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tmp</span><span class="p">])))</span>

        <span class="n">tar_tmp</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">Q_vals</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tar_tmp</span><span class="p">)</span>

        <span class="c1"># cost function (minimum time controller)</span>
        <span class="c1"># costs = 0.01</span>
        <span class="c1"># if self._morl_problem.terminal_state:</span>
        <span class="c1">#     costs += self._gamma * (1.0/reward[0])</span>
        <span class="c1">#     self._goal_hist.append(costs)</span>
        <span class="c1">#     if my_debug: log.debug(&#39;Terminal cost value in state %i is %f&#39;, state, costs)</span>
        <span class="c1"># else:</span>
        <span class="c1">#     Q_vals = []</span>
        <span class="c1">#     for i in xrange(self._morl_problem.n_actions):</span>
        <span class="c1">#         # Simulate network</span>
        <span class="c1">#         # Q_vals.append(self._net.sim(np.asarray([[state, i]])))</span>
        <span class="c1">#         tmp = np.hstack([np.array(self._morl_problem._get_position(state)), i])</span>
        <span class="c1">#         Q_vals.append(self._net.sim(np.array([tmp])))</span>
        <span class="c1">#</span>
        <span class="c1">#     costs += self._gamma * np.array(Q_vals).min()</span>
        <span class="c1">#     self._goal_hist.append(costs)</span>
        <span class="c1">#     if my_debug: log.debug(&#39;State cost value in state %i is %f&#39;, state, costs)</span>

        <span class="n">inp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_history</span><span class="p">)</span>
        <span class="n">tar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_goal_hist</span><span class="p">)</span>
        <span class="c1"># tar = tar.reshape(len(tar), 1)</span>
        <span class="n">tar</span> <span class="o">=</span> <span class="n">tar</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tar</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="c1"># Train network</span>
        <span class="c1"># error = self._net.train.train_rprop(input, target, epochs=500, show=100, goal=0.02)</span>
        <span class="n">nl</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">train_rprop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_net</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">goal</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

        <span class="c1"># Reset histories after termination of one episode</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">terminal_state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_train_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># input history for NN (s,a)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_goal_hist</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># goal history</span>

    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># epsilon greedy</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="n">Q_vals</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
                <span class="c1"># Simulate network</span>
                <span class="c1"># Q_vals.append(self._net.sim(np.asarray([[state, i]])))</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">_get_position</span><span class="p">(</span><span class="n">state</span><span class="p">)),</span> <span class="n">i</span><span class="p">])</span>
                <span class="n">Q_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_net</span><span class="o">.</span><span class="n">sim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tmp</span><span class="p">])))</span>

            <span class="n">weighted_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q_vals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scalarization_weights</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">weighted_q</span> <span class="o">==</span> <span class="nb">max</span><span class="p">(</span><span class="n">weighted_q</span><span class="p">))[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># action = random.choice(np.where(np.array(Q_vals) == min(np.array(Q_vals)))[0])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Decided for action </span><span class="si">%i</span><span class="s1"> in state </span><span class="si">%i</span><span class="s1">.&#39;</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span></div>


<div class="viewcode-block" id="MORLScalarizingAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent">[docs]</a><span class="k">class</span> <span class="nc">MORLScalarizingAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is an Agent that uses chebyshev scalarization method in Q-iteration</span>
<span class="sd">    Contains a Q-Value table with additional parameter o &lt;-- (Objective)</span>
<span class="sd">    according to: &#39;scalarized MORL: Novel design techniques&#39;</span>
<span class="sd">    @author: Simon Wölzmüller &lt;ga35voz@mytum.de&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">scalarization_weights</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">ref_point</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="s1">&#39;chebishev&#39;</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        initializes MORL Agent</span>
<span class="sd">        :param morl_problem: a Problem inheriting MORLProblem Class</span>
<span class="sd">        :param scalarization_weights: vector for weighted sum of q values, weight on important objectives</span>
<span class="sd">        :param alpha: learning rate</span>
<span class="sd">        :param epsilon: for epsilon greedy action selection</span>
<span class="sd">        :param tau: small constant addition for z point, that is used as reference and optimized during learning</span>
<span class="sd">        :param kwargs: some additional arguments</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MORLScalarizingAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># create Q-value table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span><span class="p">)</span>
        <span class="c1"># parameter for Q-learning algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="c1"># parameter for greedy strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="c1"># small constant training addition value for the z point</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="c1"># determine scalarization function:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;chebishev&#39;</span><span class="p">:</span>
            <span class="c1"># create reference point for each objective used for chebyshev scalarization adapted on each step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>

        <span class="c1"># weight vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">scalarization_weights</span>
        <span class="c1"># last action is stored</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># hypervolume calculator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span> <span class="o">=</span> <span class="n">HyperVolumeCalculator</span><span class="p">(</span><span class="n">ref_point</span><span class="p">)</span>
        <span class="c1"># stores q values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># storage for volumes per interaction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># storage for the volumes optained in ONE interaction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># scalar Q-Table:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>

<div class="viewcode-block" id="MORLScalarizingAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        prepare Agent for next use</span>
<span class="sd">        :return: nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.save"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        store q function for multiple runs</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.restore"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.restore">[docs]</a>    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        restore that q function for reuse</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.decide"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.decide">[docs]</a>    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function decides using a epsilon greedy algorithm and chebishev scalarizing function.</span>
<span class="sd">        :param state: the state the agent is at the moment</span>
<span class="sd">        :param t: iteration</span>
<span class="sd">        :return: action the agent chose</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># epsilon greedy action selection:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="c1"># greedy action selection:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># otherwise choose randomly over action space</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="c1"># log the decided action</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Decided for action </span><span class="si">%i</span><span class="s1"> in state </span><span class="si">%i</span><span class="s1">.&#39;</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.learn"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        public access to learning function</span>
<span class="sd">        :param t:</span>
<span class="sd">        :param last_state:</span>
<span class="sd">        :param action:</span>
<span class="sd">        :param reward:</span>
<span class="sd">        :param state:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># access private function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1">#  store last action after learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span></div>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        learn like they do in Van Moeffart/Drugan/Nowé paper about scalarization</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># update rule for every objective</span>
        <span class="k">for</span> <span class="n">objective</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">):</span>
            <span class="c1"># advanced bellman equation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">objective</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span><span class="p">[</span><span class="n">objective</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">new_action</span><span class="p">,</span> <span class="n">objective</span><span class="p">]</span> <span class="o">-</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">objective</span><span class="p">])</span>
            <span class="c1"># store z value</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;chebishev&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_z</span><span class="p">[</span><span class="n">objective</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">objective</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tau</span>

<div class="viewcode-block" id="MORLScalarizingAgent.get_learned_action"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.get_learned_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses epsilon greedy and weighted scalarisation for action selection</span>
<span class="sd">        :param state: state the agent is</span>
<span class="sd">        :return: action to do next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#  state -&gt; quality list</span>
        <span class="n">sq_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># explore all actions</span>
        <span class="k">for</span> <span class="n">acts</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="c1"># create value vector for objectives</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">acts</span><span class="p">,</span> <span class="p">:]]</span>
            <span class="n">sq</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">*</span><span class="n">obj</span><span class="p">[</span><span class="n">o</span><span class="p">])</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">))])</span>

            <span class="n">sq_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sq</span><span class="p">)</span>
        <span class="c1"># chosen action is the one with greatest sq value</span>
        <span class="n">new_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">sq_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_action</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.create_scalar_Q_table"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.create_scalar_Q_table">[docs]</a>    <span class="k">def</span> <span class="nf">create_scalar_Q_table</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        after learnig we can extract a qfunction.</span>
<span class="sd">        :return: nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
                <span class="n">obj</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">:]]</span>
                <span class="n">sq</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">*</span><span class="n">obj</span><span class="p">[</span><span class="n">o</span><span class="p">])</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">))])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Qs</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">sq</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.get_learned_action_gibbs_distribution"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.get_learned_action_gibbs_distribution">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses gibbs distribution to decide which action to do next</span>
<span class="sd">        :param state: given state the agent is atm</span>
<span class="sd">        :return: an array of actions to do next, with probability</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.6</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>

<div class="viewcode-block" id="MORLScalarizingAgent.name"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLScalarizingAgent.name">[docs]</a>    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">short</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        builds a name string, short or long version</span>
<span class="sd">        :param short:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">short</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="o">+</span><span class="s2">&quot;, e=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, a=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, W=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_greedy_sel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        local optimal decision</span>
<span class="sd">        depending on scalarization function</span>
<span class="sd">        also we store the q vector for hypervolume calculation</span>
<span class="sd">        :param state: current state</span>
<span class="sd">        :return: action to choose</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># state -&gt; quality list</span>
        <span class="n">sq_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># explore all actions</span>
        <span class="k">for</span> <span class="n">acts</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="c1"># create value vector for objectives</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">acts</span><span class="p">,</span> <span class="p">:]]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
                <span class="n">sq</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">*</span><span class="n">obj</span><span class="p">[</span><span class="n">o</span><span class="p">])</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">))])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;chebishev&#39;</span><span class="p">:</span>
                <span class="n">sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">*</span><span class="nb">abs</span><span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_z</span><span class="p">[</span><span class="n">o</span><span class="p">])</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">obj</span><span class="p">))])</span>
            <span class="c1"># store that value into the list</span>
            <span class="n">sq_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sq</span><span class="p">)</span>
        <span class="c1"># chosen action is the one with greatest sq value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
            <span class="n">new_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">sq_list</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s1">&#39;chebishev&#39;</span><span class="p">:</span>
            <span class="n">new_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">sq_list</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">new_action</span><span class="p">,</span> <span class="p">:]]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="c1"># store hv, only if self.l is non-empty (only this way worked for me TODO: find elegant way )</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">):</span>
            <span class="c1"># catch the list</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">)</span>
            <span class="c1"># only the points on front are needed</span>
            <span class="n">l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span><span class="o">.</span><span class="n">extract_front</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
            <span class="c1"># restore it into the member</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
            <span class="c1"># compute new hypervolume:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span><span class="o">.</span><span class="n">compute_hv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="n">remove_duplicates</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span><span class="p">)</span>
            <span class="c1"># at the end of an interaction:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">terminal_state</span><span class="p">:</span>
                <span class="c1"># store the maximum hypervolume</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span><span class="p">))</span>
                <span class="c1"># clear the temporary list</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">new_action</span></div>


<div class="viewcode-block" id="MORLHVBAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent">[docs]</a><span class="k">class</span> <span class="nc">MORLHVBAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    this class is implemenation of hypervolume based MORL agent,</span>
<span class="sd">    the reference point (ref) is used for quality evaluation of</span>
<span class="sd">    state-action lists depending on problem set.</span>
<span class="sd">    like they do in paper: &#39;Hypervolume-Based MORL&#39;, Van Moffaert, Drugan, Nowé</span>
<span class="sd">    @author: Simon Wölzmüller &lt;ga35voz@mytum.de&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">ref</span><span class="p">,</span> <span class="n">scal_weights</span><span class="p">,</span>  <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        initializes agent with params used for Q-learning and greedy decision</span>
<span class="sd">        :param morl_problem: morl problem the agent acts in</span>
<span class="sd">        :param alpha: learning rate for q learning</span>
<span class="sd">        :param: epsilon: probability of epsilon greedy algorithm</span>
<span class="sd">        :param: ref: reference point for hypervolume calculation</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MORLHVBAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># create Q-value table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span><span class="p">)</span>
        <span class="c1"># self._Q = np.random.randint(0, 100, self.q_shape)</span>

        <span class="c1"># learning rate for Q-learning algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="c1"># parameter for epsilon greedy strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="c1"># hv calculator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span> <span class="o">=</span> <span class="n">HyperVolumeCalculator</span><span class="p">(</span><span class="n">ref</span><span class="p">)</span>
        <span class="c1"># init last action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># storage for temporal volumes one for each interaction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># storage for max volumes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># storage for q values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_l</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># weights for objectives</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">scal_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Qsave</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="MORLHVBAgent.save"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        save current q table</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Qsave</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span></div>

<div class="viewcode-block" id="MORLHVBAgent.restore"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.restore">[docs]</a>    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        restore saved qtable</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q_save</span><span class="o">.</span><span class="n">__len__</span><span class="p">())</span></div>

<div class="viewcode-block" id="MORLHVBAgent.reset"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        reset agent for next use</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_l</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="MORLHVBAgent.decide"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.decide">[docs]</a>    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        epsilon greedy action selection</span>
<span class="sd">        :param t: episode</span>
<span class="sd">        :param state: state we are in</span>
<span class="sd">        :return: action to choose</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># epsilon greedy hypervolume based action selection:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="c1"># greedy action selection:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># otherwise choose randomly over action space</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">my_debug</span><span class="p">:</span>
            <span class="c1"># log the decided action</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Decided for action </span><span class="si">%i</span><span class="s1"> in state </span><span class="si">%i</span><span class="s1">.&#39;</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span></div>

<div class="viewcode-block" id="MORLHVBAgent.learn"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        this function is an adaption of the hvb q learning algorithm from van moeffart/drugan/nowé</span>
<span class="sd">        :param t: count of steps</span>
<span class="sd">        :param last_state: last state before transition to this state</span>
<span class="sd">        :param action: action to chose after this state found by HVBgreedy</span>
<span class="sd">        :param reward: reward received from this action for every objective</span>
<span class="sd">        :param state: state we&#39;re currently being in</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># access private function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1">#  store last action after learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span></div>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        private learning function</span>
<span class="sd">        like they do in paper: &#39;Hypervolume-Based MORL&#39;, Van Moffaert, Drugan, Nowé</span>
<span class="sd">        :param t:</span>
<span class="sd">        :param last_state:</span>
<span class="sd">        :param action:</span>
<span class="sd">        :param reward:</span>
<span class="sd">        :param state:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># append new q values to list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="p">:]]))</span>
        <span class="c1"># if there isnt any value yet in the list, dont calculate...</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_l</span><span class="p">):</span>
            <span class="c1"># create numpy array, needed by the hv calculator</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_l</span><span class="p">)</span>
            <span class="c1"># compute</span>
            <span class="n">l</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span><span class="o">.</span><span class="n">extract_front</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
            <span class="c1"># recreate a python list, to allow easier appending</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_l</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
        <span class="c1"># decide which state we&#39;re up to take</span>
        <span class="n">new_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># update q values</span>
        <span class="k">for</span> <span class="n">objective</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">):</span>
            <span class="c1"># advanced bellman equation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">objective</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span><span class="p">[</span><span class="n">objective</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">new_action</span><span class="p">,</span> <span class="n">objective</span><span class="p">]</span> <span class="o">-</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">objective</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_greedy_sel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        action selection strategy based on hypervolume indicator</span>
<span class="sd">        :param state:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">volumes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="c1"># store l list in local copy</span>
            <span class="n">l_set</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># append list on local copy</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_l</span><span class="p">):</span>
                <span class="n">l_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_l</span><span class="p">)</span>
            <span class="c1"># append new opjective vector</span>
            <span class="n">l_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="p">:])</span>
            <span class="c1"># append to other volumes</span>
            <span class="n">volumes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hv_calculator</span><span class="o">.</span><span class="n">compute_hv</span><span class="p">(</span><span class="n">l_set</span><span class="p">))</span>
        <span class="c1"># best action has biggest hypervolume</span>
        <span class="n">new_action</span> <span class="o">=</span> <span class="n">volumes</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">volumes</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">volumes</span><span class="p">))</span>
        <span class="c1"># if the interaction has finished</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">terminal_state</span><span class="p">:</span>
            <span class="c1"># append the maximum volume to the master volume list</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_volumes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span><span class="p">))</span>
            <span class="c1"># clear the temporary list</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">temp_vol</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">new_action</span>

    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;HVB_Q_agent_&quot;</span> <span class="o">+</span> <span class="s1">&#39;e&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;a&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span>

<div class="viewcode-block" id="MORLHVBAgent.get_learned_action_distribution"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.get_learned_action_distribution">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        scalarized greedy action selection</span>
<span class="sd">        :param state: state we&#39;re being in</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="c1"># dot product with weights</span>
            <span class="n">weighted_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span>
            <span class="c1"># the first of maximum list (if more are available)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">weighted_q</span> <span class="o">==</span> <span class="nb">max</span><span class="p">(</span><span class="n">weighted_q</span><span class="p">))[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">action</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="MORLHVBAgent.get_learned_action"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.get_learned_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses epsilon greedy and hvb action selection</span>
<span class="sd">        :param state: state the agent is</span>
<span class="sd">        :return: action to do next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">state</span><span class="p">)</span></div>

<div class="viewcode-block" id="MORLHVBAgent.get_learned_action_gibbs_distribution"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHVBAgent.get_learned_action_gibbs_distribution">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action_gibbs_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses gibbs distribution to decide which action to do next</span>
<span class="sd">        :param state: given state the agent is atm</span>
<span class="sd">        :return: an array of actions to do next, with probability</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">0.6</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">tsum</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">/</span> <span class="n">tsum</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="MORLHLearningAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHLearningAgent">[docs]</a><span class="k">class</span> <span class="nc">MORLHLearningAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average reward learning agent. Uses H-function to store model based evaluation function.</span>
<span class="sd">    Also see: &#39;MultiCriteriaAverageReward RL&#39;, S.Natarajan</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor</span>
<span class="sd">        :param morl_problem: MORL Problem to act in</span>
<span class="sd">        :param epsilon: probability for decision mechanism</span>
<span class="sd">        :param alpha: learning rate</span>
<span class="sd">        :param weights: the training weights</span>
<span class="sd">        :param kwargs: more</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MORLHLearningAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># probability for epsilon greedy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="c1"># weight vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="c1"># count of each action an state is left with</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_action_taken</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_action_taken</span><span class="p">[:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span>
        <span class="c1"># count of results an action ends with</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_action_resulted_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">))</span>
        <span class="c1"># probability of an state action state triple</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_probability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_probability</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">)</span>
        <span class="c1"># reward for state-state transition</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="c1"># h value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="c1"># scalar optimal average reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="c1"># learning rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="c1"># control variable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">False</span>

<div class="viewcode-block" id="MORLHLearningAgent.decide"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHLearningAgent.decide">[docs]</a>    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        decision making using epsilon greedy</span>
<span class="sd">        :param t: steps made so far</span>
<span class="sd">        :param state: state we&#39;re in</span>
<span class="sd">        :return: action to chose next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># with probability epsilon chose random action</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># else take the local optimal decision</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_greedy_sel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        returns local optimal action to chose</span>
<span class="sd">        :param t: steps made so far</span>
<span class="sd">        :param state: state we&#39;re being in</span>
<span class="sd">        :return: return action to chose next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">a_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># weighted sum of every actions reward</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="c1"># it&#39;s model based, so we need the probability</span>
            <span class="n">suma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_probability</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">)])</span>
            <span class="c1"># and dot product</span>
            <span class="n">a_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">suma</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="c1"># return action with max weighted average reward</span>
        <span class="k">return</span> <span class="n">a_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">a_list</span><span class="p">))</span>

<div class="viewcode-block" id="MORLHLearningAgent.learn"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHLearningAgent.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        public access for learning function</span>
<span class="sd">        :param t: steps so far</span>
<span class="sd">        :param last_state: last state we were visiting</span>
<span class="sd">        :param action: action we chose</span>
<span class="sd">        :param reward: reward we received</span>
<span class="sd">        :param state: state we&#39;re now in</span>
<span class="sd">        :return: nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># access private function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1">#  store last action after learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span></div>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        learning function</span>
<span class="sd">        :param t: episodes so far</span>
<span class="sd">        :param last_state:</span>
<span class="sd">        :param action:</span>
<span class="sd">        :param reward:</span>
<span class="sd">        :param state:</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># count up action taken from last state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_action_taken</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># count up state resulted from this state action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_action_resulted_in</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_probability</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_action_resulted_in</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">]</span> <span class="o">/</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">n_action_taken</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="o">/</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">n_action_taken</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_h</span><span class="p">[</span><span class="n">last_state</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_h</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_h_value</span><span class="p">(</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_h_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        computes the h function at this state and action</span>
<span class="sd">        :param state: state we&#39;re in now</span>
<span class="sd">        :param action: action we chose</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">h_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># compute the weighted average reward plus the h value of next statte</span>
        <span class="n">h_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_probability</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">]</span> <span class="o">*</span>
                                                                    <span class="bp">self</span><span class="o">.</span><span class="n">_h</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="p">:]</span>
                                                                    <span class="k">for</span> <span class="n">next_state</span>
                                                                    <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
        <span class="c1"># the maximal h value is stored minus the average reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_h</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">h_list</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span>

<div class="viewcode-block" id="MORLHLearningAgent.get_learned_action"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLHLearningAgent.get_learned_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses greedy and h action selection</span>
<span class="sd">        :param state: state the agent is</span>
<span class="sd">        :return: action to do next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># get action out of max q value of n_objective-dimensional matrix</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MORLRLearningAgent"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLRLearningAgent">[docs]</a><span class="k">class</span> <span class="nc">MORLRLearningAgent</span><span class="p">(</span><span class="n">MorlAgent</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Also see: &#39;MultiCriteriaAverageReward RL&#39;, S.Natarajan</span>
<span class="sd">    This class contains an average Reward optimizing agent.</span>
<span class="sd">    The R Learning agent is the model free version of H-learning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">morl_problem</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor</span>
<span class="sd">        :param morl_problem: MORL Problem, the agent is acting in</span>
<span class="sd">        :param epsilon: probability for epsilon greedy decision making</span>
<span class="sd">        :param alpha: learning rate I</span>
<span class="sd">        :param beta: learning rate II</span>
<span class="sd">        :param weights: weights for objectives</span>
<span class="sd">        :param kwargs: other arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MORLRLearningAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">morl_problem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># probability for epsilon greedy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="c1"># weight vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="c1"># learning rate # 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="c1"># reward for state-state transition</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="c1"># R value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">))</span>
        <span class="c1"># scalar optimal average reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">reward_dimension</span><span class="p">)</span>
        <span class="c1"># control for &quot;best action token&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="c1"># control parameter for learning mechanism</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">False</span>

<div class="viewcode-block" id="MORLRLearningAgent.decide"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLRLearningAgent.decide">[docs]</a>    <span class="k">def</span> <span class="nf">decide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        this function is epsilon greedy decision making</span>
<span class="sd">        :param t: time (not needed here)</span>
<span class="sd">        :param state: current state, the agent is in</span>
<span class="sd">        :return: action to choose</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># decide for local optimal action:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># else random</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_greedy_sel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        local optimal decision</span>
<span class="sd">        :param t: time, not needed</span>
<span class="sd">        :param state: state the agent is in</span>
<span class="sd">        :return: action to chose</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">a_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># evaluate weighted reward for every action</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="n">a_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span>
        <span class="c1"># to know we were chosing greedy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="c1"># return maximizing action</span>
        <span class="k">return</span> <span class="n">a_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">a_list</span><span class="p">))</span>

<div class="viewcode-block" id="MORLRLearningAgent.learn"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLRLearningAgent.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        public access to learning function</span>
<span class="sd">        :param t: steps done so far</span>
<span class="sd">        :param last_state: last state we were in</span>
<span class="sd">        :param action: action we chose</span>
<span class="sd">        :param reward: reward we got</span>
<span class="sd">        :param state: state we resulted in</span>
<span class="sd">        :return: nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># access private function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learn</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1">#  store last action after learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_action</span> <span class="o">=</span> <span class="n">action</span></div>

    <span class="k">def</span> <span class="nf">_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        private learning function, updating R function</span>
<span class="sd">        :param t: steps done so far</span>
<span class="sd">        :param last_state: last state we were in</span>
<span class="sd">        :param action: action we chose</span>
<span class="sd">        :param reward: reward we received</span>
<span class="sd">        :param state: state we resulted in</span>
<span class="sd">        :return: nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># compute weighted Reward for every action</span>
        <span class="n">rew</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_morl_problem</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)]</span>
        <span class="c1"># chose action with max weighted reward</span>
        <span class="n">maxa</span> <span class="o">=</span> <span class="n">rew</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">rew</span><span class="p">))</span>
        <span class="c1"># update rule for R function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">)</span> <span class="o">+</span>\
            <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">maxa</span><span class="p">,</span> <span class="p">:])</span>
        <span class="c1"># adaptation if we took greedy action</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">took_greedy</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="c1"># update average reward</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">last_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_R</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">maxa</span><span class="p">,</span> <span class="p">:])</span> <span class="o">+</span>\
                <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
            <span class="c1"># update learning rate</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.001</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">))</span>

<div class="viewcode-block" id="MORLRLearningAgent.get_learned_action"><a class="viewcode-back" href="../Agents.html#morl_agents.MORLRLearningAgent.get_learned_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_learned_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        uses epsilon greedy and hvb action selection</span>
<span class="sd">        :param state: state the agent is</span>
<span class="sd">        :return: action to do next</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># get action out of max q value of n_objective-dimensional matrix</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_greedy_sel</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">MORLBENCH 1.0.1 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2016, Dominik Meyer, Johannes Feldmaier, Simon Woelzmueller.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.4.1.
    </div>
  </body>
</html>