% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\else\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}


\title{MORLBENCH Documentation}
\date{Aug 28, 2016}
\release{1.0.1}
\author{Dominik Meyer, Johannes Feldmaier, Simon Woelzmueller}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


This Benchmark Tool contains lots of algorithms and szenarios to compare and analyse.

Contents:


\chapter{Multiple Objective RL Agents}
\label{Agents:welcome-to-morlbench-s-documentation}\label{Agents:multiple-objective-rl-agents}\label{Agents:module-morl_agents}\label{Agents::doc}\index{morl\_agents (module)}
Created on Nov 19, 2012

@author: Dominik Meyer \textless{}\href{mailto:meyerd@mytum.de}{meyerd@mytum.de}\textgreater{}
\index{FixedPolicyAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.FixedPolicyAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{FixedPolicyAgent}}{\emph{morl\_problem}, \emph{policy}, \emph{**kwargs}}{}
An agent, that follows a fixed policy, defined as a morl policy object.
No learning implemented.
\index{reset() (morl\_agents.FixedPolicyAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.FixedPolicyAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent!

\end{fulllineitems}


\end{fulllineitems}

\index{MORLHLearningAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHLearningAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{MORLHLearningAgent}}{\emph{morl\_problem}, \emph{epsilon}, \emph{alpha}, \emph{weights}, \emph{**kwargs}}{}
Average reward learning agent. Uses H-function to store model based evaluation function.
Also see: `MultiCriteriaAverageReward RL', S.Natarajan
\index{decide() (morl\_agents.MORLHLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHLearningAgent.decide}\pysiglinewithargsret{\bfcode{decide}}{\emph{t}, \emph{state}}{}
decision making using epsilon greedy
:param t: steps made so far
:param state: state we're in
:return: action to chose next

\end{fulllineitems}

\index{get\_learned\_action() (morl\_agents.MORLHLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHLearningAgent.get_learned_action}\pysiglinewithargsret{\bfcode{get\_learned\_action}}{\emph{state}}{}
uses greedy and h action selection
:param state: state the agent is
:return: action to do next

\end{fulllineitems}

\index{learn() (morl\_agents.MORLHLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHLearningAgent.learn}\pysiglinewithargsret{\bfcode{learn}}{\emph{t}, \emph{last\_state}, \emph{action}, \emph{reward}, \emph{state}}{}
public access for learning function
:param t: steps so far
:param last\_state: last state we were visiting
:param action: action we chose
:param reward: reward we received
:param state: state we're now in
:return: nothing

\end{fulllineitems}


\end{fulllineitems}

\index{MORLHVBAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{MORLHVBAgent}}{\emph{morl\_problem}, \emph{alpha}, \emph{epsilon}, \emph{ref}, \emph{scal\_weights}, \emph{**kwargs}}{}
this class is implemenation of hypervolume based MORL agent,
the reference point (ref) is used for quality evaluation of
state-action lists depending on problem set.
like they do in paper: `Hypervolume-Based MORL', Van Moffaert, Drugan, Nowé
@author: Simon Wölzmüller \textless{}\href{mailto:ga35voz@mytum.de}{ga35voz@mytum.de}\textgreater{}
\index{decide() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.decide}\pysiglinewithargsret{\bfcode{decide}}{\emph{t}, \emph{state}}{}
epsilon greedy action selection
:param t: episode
:param state: state we are in
:return: action to choose

\end{fulllineitems}

\index{get\_learned\_action() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.get_learned_action}\pysiglinewithargsret{\bfcode{get\_learned\_action}}{\emph{state}}{}
uses epsilon greedy and hvb action selection
:param state: state the agent is
:return: action to do next

\end{fulllineitems}

\index{get\_learned\_action\_distribution() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.get_learned_action_distribution}\pysiglinewithargsret{\bfcode{get\_learned\_action\_distribution}}{\emph{state}}{}
scalarized greedy action selection
:param state: state we're being in
:return:

\end{fulllineitems}

\index{get\_learned\_action\_gibbs\_distribution() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.get_learned_action_gibbs_distribution}\pysiglinewithargsret{\bfcode{get\_learned\_action\_gibbs\_distribution}}{\emph{state}}{}
uses gibbs distribution to decide which action to do next
:param state: given state the agent is atm
:return: an array of actions to do next, with probability

\end{fulllineitems}

\index{learn() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.learn}\pysiglinewithargsret{\bfcode{learn}}{\emph{t}, \emph{last\_state}, \emph{action}, \emph{reward}, \emph{state}}{}
this function is an adaption of the hvb q learning algorithm from van moeffart/drugan/nowé
:param t: count of steps
:param last\_state: last state before transition to this state
:param action: action to chose after this state found by HVBgreedy
:param reward: reward received from this action for every objective
:param state: state we're currently being in
:return:

\end{fulllineitems}

\index{reset() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
reset agent for next use
:return:

\end{fulllineitems}

\index{restore() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.restore}\pysiglinewithargsret{\bfcode{restore}}{}{}
restore saved qtable
:return:

\end{fulllineitems}

\index{save() (morl\_agents.MORLHVBAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLHVBAgent.save}\pysiglinewithargsret{\bfcode{save}}{}{}
save current q table
:return:

\end{fulllineitems}


\end{fulllineitems}

\index{MORLRLearningAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLRLearningAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{MORLRLearningAgent}}{\emph{morl\_problem}, \emph{epsilon}, \emph{alpha}, \emph{beta}, \emph{weights}, \emph{**kwargs}}{}
Also see: `MultiCriteriaAverageReward RL', S.Natarajan
This class contains an average Reward optimizing agent.
The R Learning agent is the model free version of H-learning
\index{decide() (morl\_agents.MORLRLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLRLearningAgent.decide}\pysiglinewithargsret{\bfcode{decide}}{\emph{t}, \emph{state}}{}
this function is epsilon greedy decision making
:param t: time (not needed here)
:param state: current state, the agent is in
:return: action to choose

\end{fulllineitems}

\index{get\_learned\_action() (morl\_agents.MORLRLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLRLearningAgent.get_learned_action}\pysiglinewithargsret{\bfcode{get\_learned\_action}}{\emph{state}}{}
uses epsilon greedy and hvb action selection
:param state: state the agent is
:return: action to do next

\end{fulllineitems}

\index{learn() (morl\_agents.MORLRLearningAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLRLearningAgent.learn}\pysiglinewithargsret{\bfcode{learn}}{\emph{t}, \emph{last\_state}, \emph{action}, \emph{reward}, \emph{state}}{}
public access to learning function
:param t: steps done so far
:param last\_state: last state we were in
:param action: action we chose
:param reward: reward we got
:param state: state we resulted in
:return: nothing

\end{fulllineitems}


\end{fulllineitems}

\index{MORLScalarizingAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{MORLScalarizingAgent}}{\emph{morl\_problem}, \emph{scalarization\_weights}, \emph{alpha}, \emph{epsilon}, \emph{tau}, \emph{ref\_point}, \emph{function='chebishev'}, \emph{gamma=0.9}, \emph{**kwargs}}{}
This class is an Agent that uses chebyshev scalarization method in Q-iteration
Contains a Q-Value table with additional parameter o \textless{}-- (Objective)
according to: `scalarized MORL: Novel design techniques'
@author: Simon Wölzmüller \textless{}\href{mailto:ga35voz@mytum.de}{ga35voz@mytum.de}\textgreater{}
\index{create\_scalar\_Q\_table() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.create_scalar_Q_table}\pysiglinewithargsret{\bfcode{create\_scalar\_Q\_table}}{}{}
after learnig we can extract a qfunction.
:return: nothing

\end{fulllineitems}

\index{decide() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.decide}\pysiglinewithargsret{\bfcode{decide}}{\emph{t}, \emph{state}}{}
This function decides using a epsilon greedy algorithm and chebishev scalarizing function.
:param state: the state the agent is at the moment
:param t: iteration
:return: action the agent chose

\end{fulllineitems}

\index{get\_learned\_action() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.get_learned_action}\pysiglinewithargsret{\bfcode{get\_learned\_action}}{\emph{state}}{}
uses epsilon greedy and weighted scalarisation for action selection
:param state: state the agent is
:return: action to do next

\end{fulllineitems}

\index{get\_learned\_action\_gibbs\_distribution() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.get_learned_action_gibbs_distribution}\pysiglinewithargsret{\bfcode{get\_learned\_action\_gibbs\_distribution}}{\emph{state}}{}
uses gibbs distribution to decide which action to do next
:param state: given state the agent is atm
:return: an array of actions to do next, with probability

\end{fulllineitems}

\index{learn() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.learn}\pysiglinewithargsret{\bfcode{learn}}{\emph{t}, \emph{last\_state}, \emph{action}, \emph{reward}, \emph{state}}{}
public access to learning function
:param t:
:param last\_state:
:param action:
:param reward:
:param state:
:return:

\end{fulllineitems}

\index{name() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.name}\pysiglinewithargsret{\bfcode{name}}{\emph{short=False}}{}
builds a name string, short or long version
:param short:
:return:

\end{fulllineitems}

\index{reset() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepare Agent for next use
:return: nothing

\end{fulllineitems}

\index{restore() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.restore}\pysiglinewithargsret{\bfcode{restore}}{}{}
restore that q function for reuse
:return:

\end{fulllineitems}

\index{save() (morl\_agents.MORLScalarizingAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MORLScalarizingAgent.save}\pysiglinewithargsret{\bfcode{save}}{}{}
store q function for multiple runs
:return:

\end{fulllineitems}


\end{fulllineitems}

\index{MorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{MorlAgent}}{\emph{morl\_problem}, \emph{**kwargs}}{}
A agent that should interface with a MORL problem.
\index{decide() (morl\_agents.MorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MorlAgent.decide}\pysiglinewithargsret{\bfcode{decide}}{\emph{t}, \emph{state}}{}
Decide which action to take in interaction
cycle t.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{t}} -- Interaction cycle we are currently in

\item {} 
\textbf{\texttt{state}} -- state we are in

\end{itemize}

\end{description}\end{quote}

action: The action to do next

\end{fulllineitems}

\index{learn() (morl\_agents.MorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.MorlAgent.learn}\pysiglinewithargsret{\bfcode{learn}}{\emph{t}, \emph{last\_state}, \emph{action}, \emph{reward}, \emph{state}}{}
Learn from the last interaction, if we have
a dynamically learning agent.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{t}} -- int Interaction cycle.

\item {} 
\textbf{\texttt{last\_state}} -- Last state where we came from

\item {} 
\textbf{\texttt{action}} -- last interaction action

\item {} 
\textbf{\texttt{reward}} -- received reward vector

\item {} 
\textbf{\texttt{state}} -- next state transited to

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{NFQAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.NFQAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{NFQAgent}}{\emph{morl\_problem}, \emph{scalarization\_weights}, \emph{gamma}, \emph{epsilon}, \emph{**kwargs}}{}
Implements neural fitted-Q iteration

Can be used with scenarios where the reward vector contains only
values between -1 and 1.

TODO: Currently only morl problems with a cartesian coordinate
system as state can be used.

\end{fulllineitems}

\index{PreScalarizedQMorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.PreScalarizedQMorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{PreScalarizedQMorlAgent}}{\emph{problem}, \emph{scalarization\_weights}, \emph{alpha=0.3}, \emph{epsilon=1.0}, \emph{**kwargs}}{}
A MORL agent, that uses Q learning with a scalar
value function, scalarizing on every learning step
\index{reset() (morl\_agents.PreScalarizedQMorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.PreScalarizedQMorlAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent! Be careful and save learned Q matrix first!

\end{fulllineitems}


\end{fulllineitems}

\index{QMorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.QMorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{QMorlAgent}}{\emph{problem}, \emph{scalarization\_weights}, \emph{alpha=0.3}, \emph{epsilon=1.0}, \emph{**kwargs}}{}
A MORL agent, that uses Q learning.
\index{reset() (morl\_agents.QMorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.QMorlAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent! Be careful and save learned Q matrix first!

\end{fulllineitems}


\end{fulllineitems}

\index{SARSALambdaMorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.SARSALambdaMorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{SARSALambdaMorlAgent}}{\emph{problem}, \emph{scalarization\_weights}, \emph{alpha=0.3}, \emph{epsilon=1.0}, \emph{lmbda=0.7}, \emph{**kwargs}}{}
SARSA with eligibility traces.
\index{reset() (morl\_agents.SARSALambdaMorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.SARSALambdaMorlAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent! Be careful and save learned Q matrix and the eligibility traces first!

\end{fulllineitems}


\end{fulllineitems}

\index{SARSAMorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.SARSAMorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{SARSAMorlAgent}}{\emph{problem}, \emph{scalarization\_weights}, \emph{alpha=0.3}, \emph{epsilon=1.0}, \emph{**kwargs}}{}
A MORL agent, that uses RL.
\index{reset() (morl\_agents.SARSAMorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.SARSAMorlAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent! Be careful and save learned Q matrix first!

\end{fulllineitems}


\end{fulllineitems}

\index{TDMorlAgent (class in morl\_agents)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.TDMorlAgent}\pysiglinewithargsret{\strong{class }\code{morl\_agents.}\bfcode{TDMorlAgent}}{\emph{problem}, \emph{scalarization\_weights}, \emph{policy}, \emph{alpha=0.3}, \emph{**kwargs}}{}
A MORL agent, that uses TD for Policy Evaluation.
\index{reset() (morl\_agents.TDMorlAgent method)}

\begin{fulllineitems}
\phantomsection\label{Agents:morl_agents.TDMorlAgent.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
resets the current agent! Be careful and save value function first!

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{Agents:module-morl_agents_multiple_criteria}\index{morl\_agents\_multiple\_criteria (module)}
Created on Mai 25 2015

@author: Simon Wölzmüller \textless{}\href{mailto:ga35voz@mytum.de}{ga35voz@mytum.de}\textgreater{}
\begin{quote}\begin{description}
\item[{members}] \leavevmode
\end{description}\end{quote}


\chapter{Multiple Objective RL Problems}
\label{Problems:multiple-objective-rl-problems}\label{Problems::doc}\phantomsection\label{Problems:module-morl_problems}\index{morl\_problems (module)}
Created on Nov 19, 2012

@author: Dominik Meyer \textless{}\href{mailto:meyerd@mytum.de}{meyerd@mytum.de}\textgreater{}
@author: Johannes Feldmaier \textless{}\href{mailto:johannes.feldmaier@tum.de}{johannes.feldmaier@tum.de}\textgreater{}
@author: Simon Wölzmüller   \textless{}\href{mailto:ga35voz@mytum.de}{ga35voz@mytum.de}\textgreater{}
\index{Deepsea (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Deepsea}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{Deepsea}}{\emph{scene=None}, \emph{actions=None}, \emph{gamma=0.9}, \emph{state=0}, \emph{extended\_reward=False}}{}
This class represents a Deepsea problem.
All the parameters should be set up on object
creation. Then the Deepsea problem can be used
iteratively by calling ``action''.
\index{play() (morl\_problems.Deepsea method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Deepsea.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
Perform an action with the submarine
and receive reward (or not).
\begin{description}
\item[{action: integer, Which action will be chosen}] \leavevmode
the agent. (0: left, 1: right, 2: up, 3: down).

\end{description}

reward: reward of the current state.

\end{fulllineitems}


\end{fulllineitems}

\index{Financial (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Financial}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{Financial}}{\emph{gamma=0.9}}{}
This is a multi-objective financial toy problem.
The agent can choose to invest into a number of financial products. Each action means to buy
or to sell one specific financial product. Therefore, if we have f financial products, there
will be 2*f actions. The problem is formulated in a bandit-style setting. This means, we have
only one state, in which we remain and can choose to buy or sell assets. The vector-valued
reward contains the payouts for the current portfolio with respect to

(payout, risk, flexibility)
\index{play() (morl\_problems.Financial method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Financial.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
Perform an action (buy or sell an asset)
\begin{description}
\item[{action: integer, which asset will be bought/sold by}] \leavevmode
the agent.

\end{description}

reward: reward of the current state.

\end{fulllineitems}


\end{fulllineitems}

\index{Gridworld (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Gridworld}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{Gridworld}}{\emph{size=10}, \emph{gamma=0.9}}{}
Original Simple-MORL-Gridworld.
\index{reset() (morl\_problems.Gridworld method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.Gridworld.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepares environment for next episode
:return: nothing

\end{fulllineitems}


\end{fulllineitems}

\index{MOPuddleworldProblem (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MOPuddleworldProblem}}{\emph{size=20}, \emph{gamma=0.9}}{}
This problem contains a quadratic map (please use size more than 15, to get a useful puddle)
the puddle is an obstacle that the agent has to drive around. The aim is to reach the goal state at the top right
\index{name() (morl\_problems.MOPuddleworldProblem static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.name}\pysiglinewithargsret{\strong{static }\bfcode{name}}{}{}
returns a simple name string
:return: name

\end{fulllineitems}

\index{play() (morl\_problems.MOPuddleworldProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
perform an action
:param action: action to perform
:return: reward for this action

\end{fulllineitems}

\index{plot\_map() (morl\_problems.MOPuddleworldProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.plot_map}\pysiglinewithargsret{\bfcode{plot\_map}}{}{}
plot a map of cartesian states
:return: a array of the flat map

\end{fulllineitems}

\index{print\_map() (morl\_problems.MOPuddleworldProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.print_map}\pysiglinewithargsret{\bfcode{print\_map}}{\emph{pos=None}}{}
plots a map of states and grid
:param pos:
:return:

\end{fulllineitems}

\index{reset() (morl\_problems.MOPuddleworldProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepare environment for new episode
:return:

\end{fulllineitems}

\index{scene\_x\_dim (morl\_problems.MOPuddleworldProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.scene_x_dim}\pysigline{\bfcode{scene\_x\_dim}}
x dimensional states count
:return: count

\end{fulllineitems}

\index{scene\_y\_dim (morl\_problems.MOPuddleworldProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MOPuddleworldProblem.scene_y_dim}\pysigline{\bfcode{scene\_y\_dim}}
y dimensional states count
:return: count

\end{fulllineitems}


\end{fulllineitems}

\index{MORLBuridansAss1DProblem (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAss1DProblem}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLBuridansAss1DProblem}}{\emph{size=3}, \emph{p=0.9}, \emph{n\_appear=10}, \emph{gamma=0.9}}{}
This problem contains buridans ass domain. An ass starts (usually) in a 3x3 grid in the middle position (1,1)
in the top left and the bottom right corner there is a pile of food. if the ass moves away from a visible foodstate,
the food in the bigger distance will be stolen with a probability of p. Eeating the food means choosing action
``stay'' at the field of a food
it will be rewarded with following criteria: hunger, lost food, walking distance
hunger means

\end{fulllineitems}

\index{MORLBuridansAssProblem (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLBuridansAssProblem}}{\emph{size=3}, \emph{p=0.9}, \emph{n\_appear=10}, \emph{gamma=1.0}}{}
This problem contains buridans ass domain. An ass starts (usually) in a 3x3 grid in the middle position (1,1)
in the top left and the bottom right corner there is a pile of food. if the ass moves away from a visible foodstate,
the food in the bigger distance will be stolen with a probability of p. Eeating the food means choosing action
``stay'' at the field of a food
it will be rewarded with following criteria: hunger, lost food, walking distance
hunger means that after 9 steps without eating a food pile the next action(s) will be rewarded with -1
\index{create\_plottable\_states() (morl\_problems.MORLBuridansAssProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.create_plottable_states}\pysiglinewithargsret{\bfcode{create\_plottable\_states}}{\emph{states}}{}
after learing we need to map the states to the cartesian coordinates
:param states: multidimensional state indices
:return: states in 2 dim indices

\end{fulllineitems}

\index{get\_cartesian\_coordinates\_from\_pos\_state() (morl\_problems.MORLBuridansAssProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.get_cartesian_coordinates_from_pos_state}\pysiglinewithargsret{\bfcode{get\_cartesian\_coordinates\_from\_pos\_state}}{\emph{state}}{}
multidimensional --\textgreater{} two dimensional state positions
:param state: 3D coordinates (pos, hunger, food)
:return: position (y, x)

\end{fulllineitems}

\index{name() (morl\_problems.MORLBuridansAssProblem static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.name}\pysiglinewithargsret{\strong{static }\bfcode{name}}{}{}
return simple name
:return: name

\end{fulllineitems}

\index{play() (morl\_problems.MORLBuridansAssProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
perform this action
:param action: this action
:return: reward for this action

\end{fulllineitems}

\index{print\_map() (morl\_problems.MORLBuridansAssProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.print_map}\pysiglinewithargsret{\bfcode{print\_map}}{\emph{pos=None}}{}
show a plot of that scene
:param pos: if you want to highlight one field, give it.
:return: nothing

\end{fulllineitems}

\index{reset() (morl\_problems.MORLBuridansAssProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepare environment for next episode
:return:

\end{fulllineitems}

\index{scene\_x\_dim (morl\_problems.MORLBuridansAssProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.scene_x_dim}\pysigline{\bfcode{scene\_x\_dim}}
x dimensional states count
:return: count

\end{fulllineitems}

\index{scene\_y\_dim (morl\_problems.MORLBuridansAssProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLBuridansAssProblem.scene_y_dim}\pysigline{\bfcode{scene\_y\_dim}}
y dimensional states count
:return: count

\end{fulllineitems}


\end{fulllineitems}

\index{MORLGridworld (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworld}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLGridworld}}{\emph{size=10}, \emph{gamma=0.9}}{}
Multiobjective gridworld.
\index{name() (morl\_problems.MORLGridworld static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworld.name}\pysiglinewithargsret{\strong{static }\bfcode{name}}{}{}
short name
:return: simple string of name

\end{fulllineitems}

\index{play() (morl\_problems.MORLGridworld method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworld.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
simulate action
:param action: one of the four specified actions
:return: reward for that action

\end{fulllineitems}


\end{fulllineitems}

\index{MORLGridworldStatic (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworldStatic}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLGridworldStatic}}{\emph{size=10}, \emph{gamma=0.9}}{}
Multiobjective gridworld.
\index{play() (morl\_problems.MORLGridworldStatic method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworldStatic.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
perform defined action
:param action: action to perform
:return: reward vector for that action

\end{fulllineitems}


\end{fulllineitems}

\index{MORLGridworldTime (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworldTime}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLGridworldTime}}{\emph{size=10}, \emph{gamma=0.9}}{}
Multiobjective gridworld.
\index{play() (morl\_problems.MORLGridworldTime method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLGridworldTime.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
do that action
:param action: action to perform
:return: reward of this action

\end{fulllineitems}


\end{fulllineitems}

\index{MORLResourceGatheringProblem (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLResourceGatheringProblem}}{\emph{size=5}, \emph{gamma=0.9}, \emph{p=0.1}}{}
In this problem the agent has to find the resources and bring them back to the homebase.
the enemies steal resources with a probability of 0.9
\index{create\_plottable\_states() (morl\_problems.MORLResourceGatheringProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.create_plottable_states}\pysiglinewithargsret{\bfcode{create\_plottable\_states}}{\emph{states}}{}
after learning, convert states into 2 dim. map states
:param states: the multidimensional states
:return: 2 dim states

\end{fulllineitems}

\index{get\_bag\_index() (morl\_problems.MORLResourceGatheringProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.get_bag_index}\pysiglinewithargsret{\bfcode{get\_bag\_index}}{\emph{bag}}{}
get the index of bag state: 0:00, 1:01, 2:10, 3:11
:param bag: bagstate
:return: index

\end{fulllineitems}

\index{name() (morl\_problems.MORLResourceGatheringProblem static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.name}\pysiglinewithargsret{\strong{static }\bfcode{name}}{}{}
return simple name string
:return: string name

\end{fulllineitems}

\index{play() (morl\_problems.MORLResourceGatheringProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
perform one action
:param action: index of the action
:return: reward vector of that action

\end{fulllineitems}

\index{print\_map() (morl\_problems.MORLResourceGatheringProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.print_map}\pysiglinewithargsret{\bfcode{print\_map}}{\emph{pos=None}}{}
print a state map
:param pos: highlighted field
:return: nothing

\end{fulllineitems}

\index{reset() (morl\_problems.MORLResourceGatheringProblem method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepare environment for next episode
:return:

\end{fulllineitems}

\index{scene\_x\_dim (morl\_problems.MORLResourceGatheringProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.scene_x_dim}\pysigline{\bfcode{scene\_x\_dim}}
count of x states
:return: count

\end{fulllineitems}

\index{scene\_y\_dim (morl\_problems.MORLResourceGatheringProblem attribute)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLResourceGatheringProblem.scene_y_dim}\pysigline{\bfcode{scene\_y\_dim}}
count of y states
:return: count

\end{fulllineitems}


\end{fulllineitems}

\index{MORLRobotActionPlanning (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLRobotActionPlanning}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MORLRobotActionPlanning}}{\emph{gamma=0.9}}{}
This is supposed to be a example problem for a high level task selection
in a robot, which can receive vector valued reward.
\index{play() (morl\_problems.MORLRobotActionPlanning method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MORLRobotActionPlanning.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
Perform an action

action: integer, what high-level decision should be taken

reward: reward of the current state.

\end{fulllineitems}


\end{fulllineitems}

\index{MountainCar (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MountainCar}}{\emph{acc\_fac=0.001}, \emph{cf=0.0025}, \emph{gamma=0.9}}{}
In this problem, a car tries to escape a valley by timed accelerations
Its own force is not enough to escape only by one acceleration
This MO version of the problem has three reward components
\index{car\_sim() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.car_sim}\pysiglinewithargsret{\bfcode{car\_sim}}{\emph{factor}}{}
drive the car
:param factor: right 1, no acc 0 and left -1
:return:

\end{fulllineitems}

\index{create\_plottable\_states() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.create_plottable_states}\pysiglinewithargsret{\bfcode{create\_plottable\_states}}{\emph{states}}{}
after learning we need the 1 dim states, to plot the way through the map
:param states: states in 2 dimensions
:return: plottable states

\end{fulllineitems}

\index{distance() (morl\_problems.MountainCar static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.distance}\pysiglinewithargsret{\strong{static }\bfcode{distance}}{\emph{point1}, \emph{point2}}{}
computes a euclidean distance between point1 and point 2
:param point1: point 1
:param point2: point 2
:return:

\end{fulllineitems}

\index{get\_state() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.get_state}\pysiglinewithargsret{\bfcode{get\_state}}{\emph{position}}{}
gets the index of a 2 dim state
:param position: (x, v)
:return: index

\end{fulllineitems}

\index{get\_velocities() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.get_velocities}\pysiglinewithargsret{\bfcode{get\_velocities}}{\emph{states}}{}
returns an array that contains the velocities {[}-0.07, 0.07{]}
:param states: indices
:return: real   velocities

\end{fulllineitems}

\index{name() (morl\_problems.MountainCar static method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.name}\pysiglinewithargsret{\strong{static }\bfcode{name}}{}{}
returns the name of the problem
:return: name

\end{fulllineitems}

\index{play() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
Perform an action with the car in the mountains
and receive reward (or not).
\begin{description}
\item[{action: integer, Which action will be chosen}] \leavevmode
0: no action -\textgreater{} coasting
1: forward thrust
-1: backward thrust

\end{description}

\end{fulllineitems}

\index{reset() (morl\_problems.MountainCar method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCar.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
reset the agent
:return: nothing

\end{fulllineitems}


\end{fulllineitems}

\index{MountainCarTime (class in morl\_problems)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCarTime}\pysiglinewithargsret{\strong{class }\code{morl\_problems.}\bfcode{MountainCarTime}}{\emph{acc\_fac=1e-05}, \emph{cf=0.0002}}{}
same problem as above, just other reward structure. we get -1 every step,
-1 for every acceleration, and +1 for reaching goal position
\index{play() (morl\_problems.MountainCarTime method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCarTime.play}\pysiglinewithargsret{\bfcode{play}}{\emph{action}}{}
Perform an action with the car in the
multi objective mountains and receive reward (or not).

Multi objectives: Minimize Time and accelerating Actions.
\begin{description}
\item[{action: integer, Which action will be chosen}] \leavevmode
0: no action -\textgreater{} coasting
1: forward thrust
-1: backward thrust

\end{description}

reward: reward of the current state.

\end{fulllineitems}

\index{reset() (morl\_problems.MountainCarTime method)}

\begin{fulllineitems}
\phantomsection\label{Problems:morl_problems.MountainCarTime.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
prepare environment for next episode
:return: nothing

\end{fulllineitems}


\end{fulllineitems}



\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{m}
\item {\texttt{morl\_agents}}, \pageref{Agents:module-morl_agents}
\item {\texttt{morl\_agents\_multiple\_criteria}}, \pageref{Agents:module-morl_agents_multiple_criteria}
\item {\texttt{morl\_problems}}, \pageref{Problems:module-morl_problems}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
